---
phase: 23-integration-testing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/integration/test_context_md_generation.py
autonomous: true

must_haves:
  truths:
    - "build_context() generates CONTEXT.md with all required sections"
    - "Token budget stays within ~8K total limit"
    - "Empty state (no connections, no global_state) generates valid CONTEXT.md"
    - "All sections appear in correct order"
  artifacts:
    - path: "tests/integration/test_context_md_generation.py"
      provides: "Integration tests for complete CONTEXT.md generation"
      min_lines: 200
  key_links:
    - from: "tests/integration/test_context_md_generation.py"
      to: "src/wxcode/services/prompt_builder.py"
      via: "imports PromptBuilder.build_context()"
      pattern: "from wxcode.services.prompt_builder import PromptBuilder"
---

<objective>
Create integration tests for complete CONTEXT.md generation with token budget validation.

Purpose: Validate end-to-end flow from extractors to CONTEXT.md generation, ensuring all sections present and token budget maintained.
Output: Test file `tests/integration/test_context_md_generation.py` with integration tests.
</objective>

<execution_context>
@/Users/gilberto/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gilberto/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/23-integration-testing/23-RESEARCH.md

@src/wxcode/services/prompt_builder.py
@tests/integration/test_database_connections_pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create integration tests for build_context and token validation</name>
  <files>tests/integration/test_context_md_generation.py</files>
  <action>
Create `tests/integration/test_context_md_generation.py` with comprehensive integration tests:

1. **Fixtures (dataclass mocks, no MongoDB):**

```python
from dataclasses import dataclass, field
from enum import Enum
from unittest.mock import MagicMock

# Mock Scope enum
class MockScope(Enum):
    APP = "APP"
    MODULE = "MODULE"
    REQUEST = "REQUEST"

@dataclass
class MockSchemaConnection:
    name: str
    type_code: int
    database_type: str
    driver_name: str
    source: str
    port: str
    database: str
    user: str
    extended_info: str = ""

@dataclass
class MockGlobalVariable:
    name: str
    wlanguage_type: str
    default_value: str | None = None
    scope: MockScope = MockScope.APP
    source_element: str = "Project.wwp"

@dataclass
class MockInitializationBlock:
    code: str
    dependencies: list[str] = field(default_factory=list)
    order: int = 0

@dataclass
class MockGlobalStateContext:
    variables: list[MockGlobalVariable] = field(default_factory=list)
    initialization_blocks: list[MockInitializationBlock] = field(default_factory=list)
```

Create fixtures:
- `mock_output_project`: MagicMock with name="TestProject", kb_id="507f..."
- `mock_stack`: MagicMock with all Stack fields (name, language, framework, file_structure, naming_conventions, type_mappings, model_template, imports_template)
- `mock_connections`: List of 2 MockSchemaConnection (sqlserver + mysql)
- `mock_global_state`: MockGlobalStateContext with 3 variables + 1 init block
- `mock_tables`: List of 2 table dicts with columns

2. **Test class `TestBuildContextComplete`:**
   - `test_build_context_with_all_sections`: Verify all 6 section headers present:
     - "## Database Schema"
     - "## Database Connections"
     - "## Environment Variables"
     - "## Global State"
     - "## Initialization Code"
     - "## MCP Server Integration"
   - `test_build_context_sections_in_order`: Verify sections appear in correct order
   - `test_build_context_empty_connections`: connections=None, verify default message
   - `test_build_context_empty_global_state`: global_state=None, verify default message
   - `test_build_context_empty_both`: Both None, still generates valid CONTEXT.md

3. **Test class `TestTokenBudget`:**
   Import tiktoken for token counting:
   ```python
   import tiktoken

   def count_tokens(text: str) -> int:
       enc = tiktoken.get_encoding("cl100k_base")
       return len(enc.encode(text))
   ```

   - `test_minimal_context_token_count`: Empty data, verify < 2000 tokens (instructions only)
   - `test_realistic_context_token_count`: With 2 connections, 5 variables, 1 init block, verify < 8000 tokens
   - `test_large_init_block_truncated`: Init block with 200 lines, verify truncation, measure tokens
   - `test_many_variables_token_impact`: 20 variables, verify still under budget

4. **Test class `TestContextContentValidation`:**
   - `test_project_name_in_context`: Verify project_name appears in generated content
   - `test_stack_details_in_context`: Verify stack.name, stack.language in content
   - `test_table_names_in_context`: Verify table names from mock_tables in content
   - `test_connection_hosts_in_context`: Verify connection hosts/ports in content
   - `test_variable_names_in_context`: Verify variable names in content

Use the established pattern from tests/integration/test_database_connections_pipeline.py for async tests and mocking.
  </action>
  <verify>
Run: `cd /Users/gilberto/projetos/wxk/wxcode && python -m pytest tests/integration/test_context_md_generation.py -v`
All tests pass.
  </verify>
  <done>
All integration tests pass. Token budget validated under 8K. All sections present in correct order. Empty state handling works.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add realistic data volume test</name>
  <files>tests/integration/test_context_md_generation.py</files>
  <action>
Add a test class `TestRealisticDataVolume` to the existing test file:

This tests with data volumes similar to real Linkpay_ADM project:

1. **Fixtures for realistic data:**
   - `large_mock_tables`: 20 tables, each with 5-10 columns
   - `many_mock_variables`: 15 global variables with various scopes
   - `complex_init_block`: Init block with 80 lines of code, 10 dependencies

2. **Test methods:**
   - `test_realistic_volume_completes`: build_context() doesn't error
   - `test_realistic_volume_token_budget`: Verify < 8000 tokens
   - `test_realistic_volume_all_sections_present`: All 6 sections exist
   - `test_realistic_volume_no_truncation_except_init`: Tables and variables not truncated, init may be

This validates the token budget holds with realistic data volumes.

The test should print token counts for debugging:
```python
print(f"Token count with realistic data: {count_tokens(content)}")
```
  </action>
  <verify>
Run: `cd /Users/gilberto/projetos/wxk/wxcode && python -m pytest tests/integration/test_context_md_generation.py::TestRealisticDataVolume -v -s`
All tests pass and token count printed.
  </verify>
  <done>
Realistic data volume tests pass. Token budget validated with production-like data. All sections present.
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/gilberto/projetos/wxk/wxcode
python -m pytest tests/integration/test_context_md_generation.py -v --tb=short
```

Expected: All tests pass, token counts under 8K.
</verification>

<success_criteria>
- [ ] Test file exists with 200+ lines
- [ ] All 4 test classes implemented
- [ ] Token counting uses tiktoken
- [ ] All tests pass
- [ ] Realistic data volume test confirms < 8K tokens
</success_criteria>

<output>
After completion, create `.planning/phases/23-integration-testing/23-02-SUMMARY.md`
</output>
